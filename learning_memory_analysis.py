#!/usr/bin/env python3
"""
VELOS-GPT5 í•™ìŠµ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¢…í•© ë¶„ì„
Learning Memory System Comprehensive Analysis
VELOS ì² í•™: íŒë‹¨ì€ ê¸°ë¡ìœ¼ë¡œ ì¦ëª…í•œë‹¤ (Decisions are proven by records)
"""

import os
import sys
import sqlite3
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import pandas as pd

# í•œêµ­ ì‹œê°„ ì„¤ì •
os.environ['TZ'] = 'Asia/Seoul'

class LearningMemoryAnalyzer:
    """VELOS-GPT5 í•™ìŠµ ë©”ëª¨ë¦¬ ì¢…í•© ë¶„ì„ê¸°"""
    
    def __init__(self, webapp_root: str = "/home/user/webapp"):
        self.webapp_root = Path(webapp_root)
        self.data_path = self.webapp_root / "data"
        self.memory_path = self.data_path / "memory"
        self.gpt5_monitor_path = self.webapp_root / "gpt5_monitor"
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = {
            'sqlite_databases': [],
            'json_files': [],
            'learning_records': [],
            'memory_states': [],
            'summary': {}
        }
    
    def get_korean_timestamp(self) -> str:
        """í•œêµ­ ì‹œê°„ íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜"""
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S KST')
    
    def analyze_sqlite_database(self, db_path: Path) -> Dict[str, Any]:
        """SQLite ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„"""
        analysis = {
            'path': str(db_path),
            'name': db_path.name,
            'size': db_path.stat().st_size if db_path.exists() else 0,
            'tables': [],
            'learning_content': [],
            'record_count': 0,
            'schema_info': {}
        }
        
        if not db_path.exists():
            analysis['error'] = "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            return analysis
        
        try:
            with sqlite3.connect(str(db_path)) as conn:
                cursor = conn.cursor()
                
                # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                for table_tuple in tables:
                    table_name = table_tuple[0]
                    analysis['tables'].append(table_name)
                    
                    # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ë³´
                    cursor.execute(f"PRAGMA table_info({table_name})")
                    schema = cursor.fetchall()
                    analysis['schema_info'][table_name] = [
                        {'name': col[1], 'type': col[2], 'notnull': col[3], 'pk': col[5]}
                        for col in schema
                    ]
                    
                    # ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    count = cursor.fetchone()[0]
                    analysis['record_count'] += count
                    
                    # í•™ìŠµ ê´€ë ¨ ì½˜í…ì¸  ê²€ìƒ‰ (ìŠ¤í‚¤ë§ˆì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬)
                    columns = [col['name'] for col in analysis['schema_info'][table_name]]
                    
                    if 'content' in columns:
                        # content ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
                        query = f"""SELECT * FROM {table_name} 
                                   WHERE content LIKE '%í•™ìŠµ%' OR content LIKE '%learning%' 
                                   OR content LIKE '%train%' OR content LIKE '%memory%'
                                   LIMIT 10"""
                        try:
                            cursor.execute(query)
                            learning_records = cursor.fetchall()
                            for record in learning_records:
                                analysis['learning_content'].append({
                                    'table': table_name,
                                    'record': dict(zip(columns, record))
                                })
                        except Exception as e:
                            print(f"âš ï¸ {table_name} í…Œì´ë¸” ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    
                    elif 'message_type' in columns and 'content' in columns:
                        # GPT-5 ëª¨ë‹ˆí„°ë§ í…Œì´ë¸”
                        query = f"""SELECT * FROM {table_name} 
                                   WHERE content LIKE '%í•™ìŠµ%' OR content LIKE '%learning%'
                                   OR message_type LIKE '%learn%'
                                   LIMIT 10"""
                        try:
                            cursor.execute(query)
                            learning_records = cursor.fetchall()
                            for record in learning_records:
                                analysis['learning_content'].append({
                                    'table': table_name,
                                    'record': dict(zip(columns, record))
                                })
                        except Exception as e:
                            print(f"âš ï¸ {table_name} í…Œì´ë¸” ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    
                    # memory_states í…Œì´ë¸” íŠ¹ë³„ ì²˜ë¦¬
                    if table_name == 'memory_states':
                        cursor.execute(f"SELECT * FROM {table_name} LIMIT 5")
                        memory_states = cursor.fetchall()
                        for state in memory_states:
                            analysis['learning_content'].append({
                                'table': table_name,
                                'record': dict(zip(columns, state)),
                                'type': 'memory_state'
                            })
        
        except Exception as e:
            analysis['error'] = f"ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì˜¤ë¥˜: {e}"
        
        return analysis
    
    def analyze_json_file(self, json_path: Path) -> Dict[str, Any]:
        """JSON íŒŒì¼ ë¶„ì„"""
        analysis = {
            'path': str(json_path),
            'name': json_path.name,
            'size': json_path.stat().st_size if json_path.exists() else 0,
            'structure': 'unknown',
            'record_count': 0,
            'learning_content': [],
            'sample_data': []
        }
        
        if not json_path.exists():
            analysis['error'] = "íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"
            return analysis
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                analysis['structure'] = 'array'
                analysis['record_count'] = len(data)
                
                # í•™ìŠµ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
                for i, item in enumerate(data[:50]):  # ì²˜ìŒ 50ê°œë§Œ ê²€ì‚¬
                    if isinstance(item, dict):
                        item_str = json.dumps(item, ensure_ascii=False).lower()
                        if any(keyword in item_str for keyword in ['í•™ìŠµ', 'learning', 'train', 'memory']):
                            analysis['learning_content'].append({
                                'index': i,
                                'content': item
                            })
                
                # ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ)
                analysis['sample_data'] = data[:3]
            
            elif isinstance(data, dict):
                analysis['structure'] = 'object'
                
                if 'items' in data and isinstance(data['items'], list):
                    analysis['record_count'] = len(data['items'])
                    analysis['sample_data'] = data['items'][:3]
                else:
                    analysis['record_count'] = len(data.keys())
                    analysis['sample_data'] = list(data.items())[:3]
                
                # í•™ìŠµ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
                data_str = json.dumps(data, ensure_ascii=False).lower()
                if any(keyword in data_str for keyword in ['í•™ìŠµ', 'learning', 'train', 'memory']):
                    analysis['learning_content'].append({
                        'type': 'full_object',
                        'content': data
                    })
        
        except Exception as e:
            analysis['error'] = f"JSON ë¶„ì„ ì˜¤ë¥˜: {e}"
        
        return analysis
    
    def scan_memory_files(self):
        """ë©”ëª¨ë¦¬ ê´€ë ¨ íŒŒì¼ë“¤ ìŠ¤ìº”"""
        print(f"ğŸ” ë©”ëª¨ë¦¬ íŒŒì¼ ìŠ¤ìº” ì‹œì‘: {self.memory_path}")
        
        if not self.memory_path.exists():
            print(f"âŒ ë©”ëª¨ë¦¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.memory_path}")
            return
        
        # SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ë“¤
        sqlite_files = list(self.memory_path.glob("*.db"))
        for db_file in sqlite_files:
            print(f"ğŸ“Š SQLite DB ë¶„ì„ ì¤‘: {db_file.name}")
            analysis = self.analyze_sqlite_database(db_file)
            self.analysis_results['sqlite_databases'].append(analysis)
        
        # JSON íŒŒì¼ë“¤
        json_files = list(self.memory_path.glob("*.json"))
        for json_file in json_files:
            print(f"ğŸ“‹ JSON íŒŒì¼ ë¶„ì„ ì¤‘: {json_file.name}")
            analysis = self.analyze_json_file(json_file)
            self.analysis_results['json_files'].append(analysis)
        
        # GPT-5 ëª¨ë‹ˆí„°ë§ ë””ë ‰í† ë¦¬ë„ ìŠ¤ìº”
        if self.gpt5_monitor_path.exists():
            gpt5_files = list(self.gpt5_monitor_path.glob("*.db"))
            for db_file in gpt5_files:
                print(f"ğŸ¤– GPT-5 ëª¨ë‹ˆí„° DB ë¶„ì„ ì¤‘: {db_file.name}")
                analysis = self.analyze_sqlite_database(db_file)
                self.analysis_results['sqlite_databases'].append(analysis)
    
    def generate_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
        total_sqlite_dbs = len(self.analysis_results['sqlite_databases'])
        total_json_files = len(self.analysis_results['json_files'])
        total_learning_records = 0
        total_memory_states = 0
        
        # í•™ìŠµ ë ˆì½”ë“œ ì§‘ê³„
        for db_analysis in self.analysis_results['sqlite_databases']:
            total_learning_records += len(db_analysis['learning_content'])
            for content in db_analysis['learning_content']:
                if content.get('type') == 'memory_state':
                    total_memory_states += 1
        
        for json_analysis in self.analysis_results['json_files']:
            total_learning_records += len(json_analysis['learning_content'])
        
        self.analysis_results['summary'] = {
            'timestamp': self.get_korean_timestamp(),
            'total_sqlite_databases': total_sqlite_dbs,
            'total_json_files': total_json_files,
            'total_learning_records': total_learning_records,
            'total_memory_states': total_memory_states,
            'analysis_quality': 'A+' if total_learning_records > 20 else 'B+' if total_learning_records > 10 else 'C+'
        }
    
    def print_detailed_report(self):
        """ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ§  VELOS-GPT5 í•™ìŠµ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ")
        print("="*80)
        
        # ìš”ì•½ ì •ë³´
        summary = self.analysis_results['summary']
        print(f"\nğŸ“Š ë¶„ì„ ìš”ì•½ ({summary['timestamp']})")
        print(f"â”œâ”€ SQLite ë°ì´í„°ë² ì´ìŠ¤: {summary['total_sqlite_databases']}ê°œ")
        print(f"â”œâ”€ JSON íŒŒì¼: {summary['total_json_files']}ê°œ")
        print(f"â”œâ”€ í•™ìŠµ ê´€ë ¨ ë ˆì½”ë“œ: {summary['total_learning_records']}ê°œ")
        print(f"â”œâ”€ ë©”ëª¨ë¦¬ ìƒíƒœ ë ˆì½”ë“œ: {summary['total_memory_states']}ê°œ")
        print(f"â””â”€ ë¶„ì„ í’ˆì§ˆ: {summary['analysis_quality']}")
        
        # SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„¸ ì •ë³´
        print(f"\nğŸ—„ï¸ SQLite ë°ì´í„°ë² ì´ìŠ¤ ìƒì„¸ ë¶„ì„")
        for i, db in enumerate(self.analysis_results['sqlite_databases'], 1):
            print(f"\n{i}. {db['name']} ({db['size']:,} bytes)")
            print(f"   â”œâ”€ í…Œì´ë¸” ìˆ˜: {len(db['tables'])}")
            print(f"   â”œâ”€ ì „ì²´ ë ˆì½”ë“œ ìˆ˜: {db['record_count']:,}")
            print(f"   â””â”€ í•™ìŠµ ê´€ë ¨ ë ˆì½”ë“œ: {len(db['learning_content'])}ê°œ")
            
            if db['tables']:
                print("   ğŸ“‹ í…Œì´ë¸” ëª©ë¡:")
                for table in db['tables']:
                    schema = db['schema_info'].get(table, [])
                    columns = [col['name'] for col in schema]
                    print(f"      â””â”€ {table}: {len(columns)}ê°œ ì»¬ëŸ¼ ({', '.join(columns[:3])}{'...' if len(columns) > 3 else ''})")
            
            if db['learning_content']:
                print("   ğŸ“ í•™ìŠµ ê´€ë ¨ ìƒ˜í”Œ:")
                for content in db['learning_content'][:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                    record = content['record']
                    if 'content' in record:
                        content_preview = str(record['content'])[:100] + "..." if len(str(record['content'])) > 100 else str(record['content'])
                        print(f"      â””â”€ {content['table']}: {content_preview}")
        
        # JSON íŒŒì¼ ìƒì„¸ ì •ë³´
        if self.analysis_results['json_files']:
            print(f"\nğŸ“‹ JSON íŒŒì¼ ìƒì„¸ ë¶„ì„")
            for i, json_file in enumerate(self.analysis_results['json_files'], 1):
                print(f"\n{i}. {json_file['name']} ({json_file['size']:,} bytes)")
                print(f"   â”œâ”€ êµ¬ì¡°: {json_file['structure']}")
                print(f"   â”œâ”€ ë ˆì½”ë“œ ìˆ˜: {json_file['record_count']:,}")
                print(f"   â””â”€ í•™ìŠµ ê´€ë ¨ ë ˆì½”ë“œ: {len(json_file['learning_content'])}ê°œ")
        
        # í•™ìŠµ ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ ìš”ì•½
        print(f"\nğŸ—ï¸ í•™ìŠµ ë©”ëª¨ë¦¬ ì•„í‚¤í…ì²˜ ìš”ì•½")
        print("â”œâ”€ ì‹¤ì‹œê°„ í•™ìŠµ ë°ì´í„°: GPT-5 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ")
        print("â”œâ”€ ì¥ê¸° í•™ìŠµ ì €ì¥ì†Œ: VELOS ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤")
        print("â”œâ”€ êµ¬ì¡°í™”ëœ í•™ìŠµ ë°ì´í„°: JSON í˜•íƒœ í•™ìŠµ ê¸°ë¡")
        print("â””â”€ ë©”ëª¨ë¦¬ ìƒíƒœ ì¶”ì : memory_states í…Œì´ë¸”")
        
        print(f"\nâœ… ê²°ë¡ : VELOS-GPT5 ì‹œìŠ¤í…œì—ëŠ” í¬ê´„ì ì¸ í•™ìŠµ ë©”ëª¨ë¦¬ êµ¬ì¡°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤!")
        print("   VELOS ì² í•™ì— ë”°ë¼ ëª¨ë“  í•™ìŠµ ê³¼ì •ê³¼ ë©”ëª¨ë¦¬ ìƒíƒœê°€ ì²´ê³„ì ìœ¼ë¡œ ê¸°ë¡ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
    
    def save_analysis_report(self):
        """ë¶„ì„ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        report_path = self.webapp_root / f"learning_memory_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥ë¨: {report_path}")
        return report_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ VELOS-GPT5 í•™ìŠµ ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘...")
    print(f"â° ë¶„ì„ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S KST')}")
    
    analyzer = LearningMemoryAnalyzer()
    
    # ë©”ëª¨ë¦¬ íŒŒì¼ ìŠ¤ìº”
    analyzer.scan_memory_files()
    
    # ë¶„ì„ ê²°ê³¼ ìš”ì•½ ìƒì„±
    analyzer.generate_summary()
    
    # ìƒì„¸ ë³´ê³ ì„œ ì¶œë ¥
    analyzer.print_detailed_report()
    
    # ë¶„ì„ ë³´ê³ ì„œ ì €ì¥
    report_path = analyzer.save_analysis_report()
    
    print(f"\nğŸ‰ í•™ìŠµ ë©”ëª¨ë¦¬ ë¶„ì„ ì™„ë£Œ!")
    return report_path

if __name__ == "__main__":
    main()