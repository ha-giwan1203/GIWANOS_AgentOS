# [ACTIVE] VELOS ë©”ëª¨ë¦¬ ì¸ì‚¬ì´íŠ¸ ë·°ì–´ - ë©”ëª¨ë¦¬ ë°ì´í„° ì‹œê°í™” ì‹œìŠ¤í…œ
# ---------------- Memory Insight Viewer (ê³ ë„í™”) ----------------
# ìš”êµ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬: streamlit, python-dateutil (ì„ íƒ), dotenv(ì„ íƒ)
# pip install python-dateutil python-dotenv streamlit

import os
import json
import csv
import sqlite3
import io
import re
from pathlib import Path
from datetime import datetime, timedelta

try:
    from dateutil.parser import parse as parse_dt
except Exception:
    parse_dt = None

import streamlit as st

# .env ì„ íƒ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception:
    pass

VELOS_ROOT = Path(os.getenv("VELOS_ROOT", r"C:\giwanos"))
MEM_DIR = Path(os.getenv("VELOS_MEM_DIR", str(VELOS_ROOT / "data" / "memory")))
MEM_JSONL = MEM_DIR / "learning_memory.jsonl"
MEM_SQLITE = MEM_DIR / "velos.db"
SESS_DIR = VELOS_ROOT / "data" / "sessions"
REFL_DIR = VELOS_ROOT / "data" / "reflections"


def _coerce_ts(s: str) -> datetime | None:
    if not s:
        return None
    if parse_dt:
        try:
            return parse_dt(s)
        except Exception:
            pass
    # ëŒ€ê°• YYYY-MM-DD HH:MM:SS ë˜ëŠ” ISO
    try:
        s = s.replace("T", " ").split(".")[0]
        return datetime.strptime(s, "%Y-%m-%d %H:%M:%S")
    except Exception:
        return None


def read_jsonl_tail(path: Path, limit: int = 5000) -> list[dict]:
    """JSONLì˜ ë§ˆì§€ë§‰ Në¼ì¸ì„ ì½ì–´ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    if not path.exists():
        return []
    # ë©”ëª¨ë¦¬ í­ì£¼ ë°©ì§€: ë§ˆì§€ë§‰ limitë¼ì¸ë§Œ ì½ê¸°
    # íŒŒì¼ì´ ë§¤ìš° í´ ìˆ˜ ìˆìœ¼ë‹ˆ ë‘ ë²ˆ íŒ¨ìŠ¤ í”¼í•œë‹¤
    lines = path.read_text(encoding="utf-8", errors="ignore").splitlines()
    chunk = lines[-limit:]
    out = []
    for ln in chunk:
        try:
            obj = json.loads(ln)
            # í•„ë“œ ì •ê·œí™”
            obj.setdefault("ts", obj.get("timestamp") or obj.get("time") or "")
            obj.setdefault("from", obj.get("speaker") or obj.get("role") or obj.get("source") or "")
            obj.setdefault("insight", obj.get("content") or obj.get("message") or obj.get("text") or "")
            # íƒœê·¸ íƒ€ì… ë³´ì •
            tags = obj.get("tags")
            if isinstance(tags, str):
                obj["tags"] = [t.strip() for t in re.split(r"[;,]", tags) if t.strip()]
            elif tags is None:
                obj["tags"] = []
            out.append(obj)
        except Exception:
            # ë§ê°€ì§„ ë¼ì¸ì€ ì¡°ìš©íˆ ë¬´ì‹œ
            continue
    return out


def read_sqlite_recent(db: Path, limit: int = 1000) -> list[dict]:
    if not db.exists():
        return []
    try:
        con = sqlite3.connect(str(db))
        cur = con.cursor()
        # ìŠ¤í‚¤ë§ˆê°€ í”„ë¡œì íŠ¸ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ë°©ì–´ì ìœ¼ë¡œ í•„ë“œ ë§¤í•‘
        # ê°€ëŠ¥í•œ ì»¬ëŸ¼ í›„ë³´: ts|timestamp, from|speaker|role, insight|content|text
        cols = None
        # ì •ë³´ ìŠ¤í‚¤ë§ˆ íƒìƒ‰
        cur.execute("PRAGMA table_info(memory);")
        colnames = [r[1].lower() for r in cur.fetchall()]
        # ì¹¼ëŸ¼ ë§¤í•‘
        c_ts = "ts" if "ts" in colnames else ("timestamp" if "timestamp" in colnames else None)
        c_from = "from" if "from" in colnames else ("speaker" if "speaker" in colnames else ("role" if "role" in colnames else None))
        c_text = "insight" if "insight" in colnames else ("content" if "content" in colnames else ("text" if "text" in colnames else None))
        if not all([c_ts, c_from, c_text]):
            # ìŠ¤í‚¤ë§ˆ ë‹¤ë¥´ë©´ í¬ê¸°
            con.close()
            return []
        q = f'SELECT {c_ts}, {c_from}, {c_text} FROM memory ORDER BY {c_ts} DESC LIMIT ?'
        cur.execute(q, (limit,))
        rows = [{"ts": r[0], "from": r[1], "insight": r[2]} for r in cur.fetchall()]
        con.close()
        return rows
    except Exception:
        return []


def filter_rows(rows: list[dict], speaker: str, tags: list[str], q: str, start: datetime | None, end: datetime | None) -> list[dict]:
    out = []
    pat = re.compile(re.escape(q), re.I) if q else None
    tagset = set([t.strip().lower() for t in tags if t.strip()])
    for r in rows:
        ts = _coerce_ts(r.get("ts", ""))
        if start and ts and ts < start:
            continue
        if end and ts and ts > end:
            continue
        if speaker and (r.get("from", "").lower() != speaker.lower()):
            continue
        if tagset:
            row_tags = {t.lower() for t in (r.get("tags") or [])}
            if not row_tags.intersection(tagset):
                continue
        if pat:
            blob = " ".join([
                r.get("insight", ""), r.get("from", ""),
                " ".join(r.get("tags") or [])
            ])
            if not pat.search(blob):
                continue
        out.append(r)
    return out


def paginate(rows: list[dict], page: int, page_size: int) -> list[dict]:
    s = max(0, (page - 1) * page_size)
    e = s + page_size
    return rows[s:e]


def export_csv(rows: list[dict]) -> bytes:
    buf = io.StringIO()
    w = csv.writer(buf)
    w.writerow(["ts", "from", "insight", "tags"])
    for r in rows:
        w.writerow([r.get("ts", ""), r.get("from", ""), r.get("insight", ""), ";".join(r.get("tags") or [])])
    return buf.getvalue().encode("utf-8-sig")


# ------------- UI -------------
st.set_page_config(page_title="Memory Insight Viewer", layout="wide")
st.header("ğŸ§  Memory Insight Viewer (ì‹¤ì‹œê°„ ëŒ€í™”Â·í•™ìŠµ ë¡œê·¸)")

src_tab, = st.tabs(["ì‹¤ì‹œê°„ ë¡œê·¸"])

with src_tab:
    # ì†ŒìŠ¤ ì„ íƒ
    st.markdown("#### ë°ì´í„° ì†ŒìŠ¤")
    source = st.radio("ì½ê¸° ëŒ€ìƒ", ["JSONL(ê¶Œì¥)", "SQLite"], horizontal=True, label_visibility="collapsed")

    # ì»¨íŠ¸ë¡¤ íŒ¨ë„
    st.markdown("#### í•„í„°")
    cols = st.columns([1, 1, 2, 2, 1, 1])
    with cols[0]:
        speaker = st.selectbox("ë°œí™”ì", ["", "user", "assistant", "system"])
    with cols[1]:
        last_minutes = st.selectbox("ìµœê·¼", ["", "5ë¶„", "15ë¶„", "1ì‹œê°„", "6ì‹œê°„", "24ì‹œê°„", "7ì¼"])
    with cols[2]:
        start_str = st.text_input("ì‹œì‘(YYYY-MM-DD HH:MM:SS)", value="")
    with cols[3]:
        end_str = st.text_input("ì¢…ë£Œ(YYYY-MM-DD HH:MM:SS)", value="")
    with cols[4]:
        tag_str = st.text_input("íƒœê·¸(ì‰¼í‘œ , êµ¬ë¶„)", value="")
    with cols[5]:
        q = st.text_input("í‚¤ì›Œë“œ", value="")

    # ìë™ ìƒˆë¡œê³ ì¹¨
    auto = st.toggle("ìë™ ìƒˆë¡œê³ ì¹¨(5ì´ˆ)", value=False)
    if auto:
        st.autorefresh(interval=5000, key="mem_auto_refresh")

    # í˜ì´ì§€ë„¤ì´ì…˜
    pcol = st.columns([1, 1, 2, 2, 1, 1, 1])
    with pcol[0]:
        page_size = st.selectbox("í˜ì´ì§€ í¬ê¸°", [25, 50, 100, 200], index=1)
    with pcol[1]:
        page = st.number_input("í˜ì´ì§€", min_value=1, value=1, step=1)

    # ë§í¬ íŒ¨ë„
    st.markdown("#### ê²½ë¡œ")
    cols2 = st.columns(3)
    with cols2[0]:
        st.write(f"JSONL: `{MEM_JSONL}`")
        st.write(f"SQLite: `{MEM_SQLITE}`")
    with cols2[1]:
        last_ref = None
        if REFL_DIR.exists():
            last = sorted(REFL_DIR.glob("reflection_*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
            last_ref = last[0] if last else None
        st.write(f"Reflections: `{last_ref or '(ì—†ìŒ)'}`")
    with cols2[2]:
        last_sess = None
        if SESS_DIR.exists():
            last = sorted(SESS_DIR.glob("session_*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
            last_sess = last[0] if last else None
        st.write(f"Session: `{last_sess or '(ì—†ìŒ)'}`")

    # ë°ì´í„° ë¡œë“œ
    if source.startswith("JSONL"):
        rows = read_jsonl_tail(MEM_JSONL, limit=5000)
    else:
        rows = read_sqlite_recent(MEM_SQLITE, limit=2000)

    # ì‹œê°„ í•„í„° êµ¬ë¬¸ ì²˜ë¦¬
    start = _coerce_ts(start_str) if start_str else None
    end = _coerce_ts(end_str) if end_str else None
    if last_minutes:
        now = datetime.now()
        span = {
            "5ë¶„": 5, "15ë¶„": 15, "1ì‹œê°„": 60, "6ì‹œê°„": 360, "24ì‹œê°„": 1440, "7ì¼": 10080
        }[last_minutes]
        start = now - timedelta(minutes=span)
        end = now

    # íƒœê·¸ íŒŒì‹±
    tags = [t.strip() for t in tag_str.split(",")] if tag_str.strip() else []

    # í•„í„° ì ìš©
    filtered = filter_rows(rows, speaker=speaker, tags=tags, q=q, start=start, end=end)

    # í˜ì´ì§€ë„¤ì´ì…˜
    total = len(filtered)
    page_max = max(1, (total + page_size - 1) // page_size)
    if page > page_max:
        page = page_max
    view = paginate(filtered, page=page, page_size=page_size)

    # ì•¡ì…˜ ë²„íŠ¼ë“¤
    cta = st.columns([1, 1, 1, 3])
    with cta[0]:
        st.metric("ì´ ë ˆì½”ë“œ", total)
    with cta[1]:
        if st.button("CSV ë‚´ë³´ë‚´ê¸°"):
            data = export_csv(filtered)
            st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=data, file_name="velos_memory_export.csv", mime="text/csv", use_container_width=True)
    with cta[2]:
        redact = st.toggle("ë¯¼ê°ì •ë³´ ê°€ë¦¬ê¸°(ìˆ«ì/ì´ë©”ì¼/í† í° íŒ¨í„´)", value=False)

    # ë Œë”
    st.divider()
    for r in view:
        ts = r.get("ts", "")
        sp = r.get("from", "")
        text = r.get("insight", "")
        tags_show = ", ".join(r.get("tags") or [])

        if redact:
            # ë§¤ìš° ë‹¨ìˆœí•œ ë§ˆìŠ¤í‚¹(ìš´ì˜ì¤‘ ê³¼ë„í•œ ë…¸ì´ì¦ˆ ë°©ì§€)
            text = re.sub(r"[A-Za-z0-9]{24,}", "[SECRET]", text)  # ê¸´ í† í°ë¥˜
            text = re.sub(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", "[EMAIL]", text)
            text = re.sub(r"\b\d{6,}\b", "[NUM]", text)

        st.markdown(f"**{sp or '?'}** Â· `{ts or '?'}`  \n{text}")
        if tags_show:
            st.caption(f"ğŸ· {tags_show}")
        st.markdown("---")

    # ë°”ë‹¥ ì •ë³´
    st.caption(f"í˜ì´ì§€ {page}/{page_max} Â· í‘œì‹œ {len(view)}/{total} Â· ì†ŒìŠ¤: {source}")
