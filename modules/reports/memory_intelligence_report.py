#!/usr/bin/env python3
# =========================================================
# VELOS ë©”ëª¨ë¦¬ ë¶„ì„ ì¸í…”ë¦¬ì „ìŠ¤ ë³´ê³ ì„œ ìƒì„±ê¸°
# í•™ìŠµ íŒ¨í„´, ì§€ì‹ ì¦ê°€, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì‹¬ì¸µ ë¶„ì„
# =========================================================

import json
import time
import os
import re
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from collections import defaultdict, Counter
import hashlib

# Use correct path for sandbox environment
ROOT = Path("/home/user/webapp")

class MemoryIntelligenceAnalyzer:
    """ë©”ëª¨ë¦¬ ì¸í…”ë¦¬ì „ìŠ¤ ë¶„ì„ê¸°"""
    
    def __init__(self, analysis_depth: str = "deep"):
        self.root = ROOT
        self.memory_dir = self.root / "data" / "memory"
        self.logs_dir = self.root / "data" / "logs"
        self.analysis_depth = analysis_depth  # shallow, standard, deep
        
        # í‚¤ì›Œë“œ ë¶„ë¥˜
        self.technical_keywords = {
            "development": ["python", "javascript", "code", "function", "api", "debug", "error", "fix"],
            "system": ["memory", "database", "file", "path", "config", "environment", "server"],
            "ai_ml": ["gpt", "ai", "model", "learning", "training", "neural", "intelligence"],  
            "integration": ["slack", "notion", "webhook", "integration", "bridge", "dispatch"],
            "operations": ["backup", "log", "monitor", "health", "performance", "optimization"]
        }
    
    def load_json_safe(self, file_path: Path) -> Dict[str, Any]:
        """ì•ˆì „í•œ JSON ë¡œë”©"""
        if not file_path.exists():
            return {}
        try:
            content = file_path.read_text(encoding="utf-8")
            # BOM ì œê±°
            if content.startswith('\ufeff'):
                content = content[1:]
            return json.loads(content)
        except Exception as e:
            print(f"[WARN] JSON ë¡œë”© ì‹¤íŒ¨ {file_path}: {e}")
            return {}
    
    def analyze_memory_database(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ì‹¬ì¸µ ë¶„ì„"""
        db_analysis = {
            "total_records": 0,
            "record_types": {},
            "growth_pattern": {},
            "content_analysis": {},
            "quality_metrics": {},
            "insights": []
        }
        
        # DB íŒŒì¼ ì°¾ê¸°
        db_files = list(self.root.glob("**/*.db"))
        if not db_files:
            db_analysis["insights"].append("âš ï¸ SQLite DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return db_analysis
            
        try:
            # ê°€ì¥ ìµœê·¼ DB íŒŒì¼ ì‚¬ìš©
            latest_db = max(db_files, key=lambda x: x.stat().st_mtime)
            
            with sqlite3.connect(latest_db) as conn:
                cursor = conn.cursor()
                
                # í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                
                for table_name, in tables:
                    try:
                        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                        count = cursor.fetchone()[0]
                        db_analysis["record_types"][table_name] = count
                        db_analysis["total_records"] += count
                        
                        # ë©”ì¸ ë©”ëª¨ë¦¬ í…Œì´ë¸” ë¶„ì„
                        if "memory" in table_name.lower() or "learning" in table_name.lower():
                            self._analyze_memory_table(cursor, table_name, db_analysis)
                            
                    except sqlite3.Error as e:
                        print(f"[WARN] í…Œì´ë¸” {table_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                        
        except Exception as e:
            db_analysis["insights"].append(f"âš ï¸ DB ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        return db_analysis
    
    def _analyze_memory_table(self, cursor: sqlite3.Cursor, table_name: str, analysis: Dict[str, Any]):
        """ë©”ëª¨ë¦¬ í…Œì´ë¸” ìƒì„¸ ë¶„ì„"""
        try:
            # ì»¬ëŸ¼ ì •ë³´ í™•ì¸
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = [col[1] for col in cursor.fetchall()]
            
            # ìµœê·¼ ë ˆì½”ë“œ ìƒ˜í”Œë§
            limit = 100 if self.analysis_depth == "deep" else 20
            cursor.execute(f"SELECT * FROM {table_name} ORDER BY rowid DESC LIMIT {limit}")
            recent_records = cursor.fetchall()
            
            if recent_records:
                analysis["content_analysis"][table_name] = {
                    "columns": columns,
                    "recent_sample_size": len(recent_records),
                    "content_patterns": self._extract_content_patterns(recent_records, columns)
                }
                
        except sqlite3.Error as e:
            print(f"[WARN] ë©”ëª¨ë¦¬ í…Œì´ë¸” {table_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _extract_content_patterns(self, records: List[Tuple], columns: List[str]) -> Dict[str, Any]:
        """ë ˆì½”ë“œì—ì„œ ì½˜í…ì¸  íŒ¨í„´ ì¶”ì¶œ"""
        patterns = {
            "topic_distribution": Counter(),
            "content_types": Counter(),
            "language_patterns": Counter(),
            "complexity_levels": {"simple": 0, "medium": 0, "complex": 0}
        }
        
        # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
        text_columns = []
        for i, col in enumerate(columns):
            if any(keyword in col.lower() for keyword in ["content", "text", "raw", "insight", "summary"]):
                text_columns.append(i)
        
        for record in records:
            for col_idx in text_columns:
                if col_idx < len(record) and record[col_idx]:
                    content = str(record[col_idx]).lower()
                    
                    # ì£¼ì œ ë¶„ë¥˜
                    for category, keywords in self.technical_keywords.items():
                        for keyword in keywords:
                            if keyword in content:
                                patterns["topic_distribution"][category] += 1
                    
                    # ì–¸ì–´ íŒ¨í„´
                    if re.search(r'[ê°€-í£]', content):
                        patterns["language_patterns"]["korean"] += 1
                    if re.search(r'[a-zA-Z]', content):
                        patterns["language_patterns"]["english"] += 1
                    
                    # ë³µì¡ë„ ë¶„ì„
                    word_count = len(content.split())
                    if word_count < 10:
                        patterns["complexity_levels"]["simple"] += 1
                    elif word_count < 50:
                        patterns["complexity_levels"]["medium"] += 1
                    else:
                        patterns["complexity_levels"]["complex"] += 1
        
        return patterns
    
    def analyze_jsonl_memory(self) -> Dict[str, Any]:
        """JSONL ë©”ëª¨ë¦¬ íŒŒì¼ ë¶„ì„"""
        jsonl_analysis = {
            "files_analyzed": 0,
            "total_entries": 0,
            "temporal_patterns": {},
            "content_insights": {},
            "learning_velocity": "unknown"
        }
        
        jsonl_files = list(self.memory_dir.glob("*.jsonl"))
        
        for jsonl_file in jsonl_files:
            jsonl_analysis["files_analyzed"] += 1
            
            try:
                content = jsonl_file.read_text(encoding="utf-8")
                if content.startswith('\ufeff'):
                    content = content[1:]
                    
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                jsonl_analysis["total_entries"] += len(lines)
                
                # ì‹œê°„ë³„ íŒ¨í„´ ë¶„ì„
                timestamps = []
                topics = Counter()
                
                for line in lines:
                    try:
                        entry = json.loads(line)
                        
                        # íƒ€ì„ìŠ¤íƒ¬í”„ ë¶„ì„
                        if "ts" in entry:
                            timestamps.append(entry["ts"])
                        elif "timestamp" in entry:
                            if entry["timestamp"] != "now":
                                try:
                                    ts = datetime.fromisoformat(entry["timestamp"]).timestamp()
                                    timestamps.append(ts)
                                except:
                                    pass
                        
                        # ì£¼ì œ ë¶„ì„
                        raw_content = entry.get("raw", entry.get("summary", "")).lower()
                        for category, keywords in self.technical_keywords.items():
                            for keyword in keywords:
                                if keyword in raw_content:
                                    topics[category] += 1
                                    
                    except json.JSONDecodeError:
                        continue
                
                # ì‹œê°„ íŒ¨í„´ ë¶„ì„
                if timestamps:
                    timestamps.sort()
                    if len(timestamps) > 1:
                        time_diffs = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
                        avg_interval = sum(time_diffs) / len(time_diffs) / 3600  # ì‹œê°„ ë‹¨ìœ„
                        
                        if avg_interval < 1:
                            jsonl_analysis["learning_velocity"] = "ë§¤ìš° ë¹ ë¦„ (ì‹œê°„ë‹¹ ë‹¤ìˆ˜)"
                        elif avg_interval < 24:
                            jsonl_analysis["learning_velocity"] = "ë¹ ë¦„ (ì¼ì¼ ë‹¤ìˆ˜)"
                        elif avg_interval < 168:
                            jsonl_analysis["learning_velocity"] = "ë³´í†µ (ì£¼ê°„ ë‹¤ìˆ˜)"
                        else:
                            jsonl_analysis["learning_velocity"] = "ëŠë¦¼ (ì›”ê°„ ì†Œìˆ˜)"
                
                jsonl_analysis["content_insights"]["topic_distribution"] = dict(topics.most_common(10))
                
            except Exception as e:
                print(f"[WARN] JSONL íŒŒì¼ {jsonl_file.name} ë¶„ì„ ì‹¤íŒ¨: {e}")
        
        return jsonl_analysis
    
    def analyze_learning_patterns(self) -> Dict[str, Any]:
        """í•™ìŠµ íŒ¨í„´ ë¶„ì„"""
        learning_analysis = {
            "learning_style": "unknown",
            "knowledge_domains": {},
            "retention_patterns": {},
            "learning_efficiency": "unknown",
            "recommendations": []
        }
        
        # ì‹œìŠ¤í…œ ê±´ê°• ë°ì´í„°ì—ì„œ í•™ìŠµ ì§€í‘œ ì¶”ì¶œ
        health_data = self.load_json_safe(self.logs_dir / "system_health.json")
        
        if health_data:
            # ì„¸ì…˜ ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°ì´í„°
            bootstrap_count = health_data.get("session_bootstrap_count", 0)
            flush_moved = health_data.get("session_bootstrap_flush_moved", 0)
            
            if bootstrap_count > 0 and flush_moved > 0:
                efficiency_ratio = flush_moved / bootstrap_count
                if efficiency_ratio > 5:
                    learning_analysis["learning_efficiency"] = "ë†’ìŒ"
                    learning_analysis["recommendations"].append("âœ… ë†’ì€ í•™ìŠµ íš¨ìœ¨ì„± - í˜„ì¬ íŒ¨í„´ ìœ ì§€")
                elif efficiency_ratio > 2:
                    learning_analysis["learning_efficiency"] = "ë³´í†µ"
                else:
                    learning_analysis["learning_efficiency"] = "ë‚®ìŒ"
                    learning_analysis["recommendations"].append("ğŸ’¡ í•™ìŠµ íš¨ìœ¨ì„± ê°œì„  í•„ìš” - ë” ì²´ê³„ì ì¸ ì •ë³´ ìˆ˜ì§‘")
            
            # ë©”ëª¨ë¦¬ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥
            pipeline_ok = health_data.get("memory_pipeline_last_test_ok", False)
            if pipeline_ok:
                learning_analysis["retention_patterns"]["pipeline_health"] = "ì •ìƒ"
            else:
                learning_analysis["retention_patterns"]["pipeline_health"] = "ì´ìŠˆ"
                learning_analysis["recommendations"].append("âš ï¸ ë©”ëª¨ë¦¬ íŒŒì´í”„ë¼ì¸ ì ê²€ í•„ìš”")
        
        return learning_analysis
    
    def generate_memory_insights(self, db_analysis: Dict, jsonl_analysis: Dict, learning_analysis: Dict) -> List[str]:
        """ë©”ëª¨ë¦¬ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ë°ì´í„° ë³¼ë¥¨ ì¸ì‚¬ì´íŠ¸
        total_records = db_analysis.get("total_records", 0) + jsonl_analysis.get("total_entries", 0)
        if total_records > 1000:
            insights.append(f"ğŸ“ í’ë¶€í•œ í•™ìŠµ ë°ì´í„°: ì´ {total_records:,}ê°œ ë ˆì½”ë“œ ë³´ìœ ")
        elif total_records > 100:
            insights.append(f"ğŸ“š ì ì • í•™ìŠµ ë°ì´í„°: {total_records:,}ê°œ ë ˆì½”ë“œ")
        else:
            insights.append(f"ğŸ“– ì´ˆê¸° í•™ìŠµ ë‹¨ê³„: {total_records:,}ê°œ ë ˆì½”ë“œ")
        
        # í•™ìŠµ ì†ë„ ì¸ì‚¬ì´íŠ¸
        velocity = jsonl_analysis.get("learning_velocity", "unknown")
        if velocity != "unknown":
            insights.append(f"âš¡ í•™ìŠµ ì†ë„: {velocity}")
        
        # ì£¼ì œ ë‹¤ì–‘ì„± ì¸ì‚¬ì´íŠ¸
        topics = jsonl_analysis.get("content_insights", {}).get("topic_distribution", {})
        if topics:
            top_topic = max(topics.keys(), key=lambda x: topics[x])
            insights.append(f"ğŸ¯ ì£¼ë ¥ í•™ìŠµ ë¶„ì•¼: {top_topic} ({topics[top_topic]}ê±´)")
            
            diversity_score = len(topics)
            if diversity_score >= 4:
                insights.append(f"ğŸŒˆ ë‹¤ì–‘í•œ í•™ìŠµ ì˜ì—­: {diversity_score}ê°œ ë¶„ì•¼ í™œë°œ")
            elif diversity_score >= 2:
                insights.append(f"ğŸ“Š ì§‘ì¤‘ í•™ìŠµ: {diversity_score}ê°œ ì£¼ìš” ë¶„ì•¼")
            else:
                insights.append("ğŸ¯ íŠ¹í™” í•™ìŠµ: ë‹¨ì¼ ë¶„ì•¼ ì§‘ì¤‘")
        
        # í•™ìŠµ íš¨ìœ¨ì„± ì¸ì‚¬ì´íŠ¸
        efficiency = learning_analysis.get("learning_efficiency", "unknown")
        if efficiency != "unknown":
            insights.append(f"ğŸ“ˆ í•™ìŠµ íš¨ìœ¨ì„±: {efficiency}")
        
        return insights
    
    def generate_title(self, analysis_results: Dict[str, Any]) -> str:
        """ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ë™ì  ì œëª© ìƒì„±"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        
        # ì´ ë°ì´í„°ëŸ‰ ê¸°ë°˜ ì œëª© ê²°ì •
        total_records = (analysis_results.get("database", {}).get("total_records", 0) + 
                        analysis_results.get("jsonl", {}).get("total_entries", 0))
        
        learning_efficiency = analysis_results.get("learning", {}).get("learning_efficiency", "unknown")
        
        if total_records > 1500:
            if learning_efficiency == "ë†’ìŒ":
                return f"ğŸ§  VELOS ë©”ëª¨ë¦¬ ë¶„ì„ - ê³ íš¨ìœ¨ í•™ìŠµ ì‹œìŠ¤í…œ ({timestamp})"
            else:
                return f"ğŸ“Š VELOS ë©”ëª¨ë¦¬ ë¶„ì„ - ëŒ€ìš©ëŸ‰ ë°ì´í„° í™œìš© ({timestamp})"
        elif total_records > 500:
            return f"ğŸ“ VELOS ë©”ëª¨ë¦¬ ë¶„ì„ - ì•ˆì •ì  í•™ìŠµ ì§„í–‰ ({timestamp})"
        else:
            return f"ğŸŒ± VELOS ë©”ëª¨ë¦¬ ë¶„ì„ - ì´ˆê¸° í•™ìŠµ ë‹¨ê³„ ({timestamp})"
    
    def generate_intelligence_content(self) -> Dict[str, Any]:
        """ì „ì²´ ë©”ëª¨ë¦¬ ì¸í…”ë¦¬ì „ìŠ¤ ì½˜í…ì¸  ìƒì„±"""
        print("[INFO] ë©”ëª¨ë¦¬ ì¸í…”ë¦¬ì „ìŠ¤ ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
        
        # ë¶„ì„ ì‹¤í–‰
        db_analysis = self.analyze_memory_database()
        jsonl_analysis = self.analyze_jsonl_memory()
        learning_analysis = self.analyze_learning_patterns()
        
        analysis_results = {
            "database": db_analysis,
            "jsonl": jsonl_analysis,
            "learning": learning_analysis
        }
        
        # ì œëª© ìƒì„±
        title = self.generate_title(analysis_results)
        
        # ì¸ì‚¬ì´íŠ¸ ìƒì„±
        insights = self.generate_memory_insights(db_analysis, jsonl_analysis, learning_analysis)
        
        # ì¶”ì²œì‚¬í•­ ìˆ˜ì§‘
        all_recommendations = []
        all_recommendations.extend(learning_analysis.get("recommendations", []))
        
        return {
            "title": title,
            "timestamp": datetime.now().isoformat(),
            "analysis_depth": self.analysis_depth,
            "database": db_analysis,
            "jsonl": jsonl_analysis, 
            "learning": learning_analysis,
            "insights": insights,
            "recommendations": all_recommendations
        }

def generate_memory_intelligence_report(depth: str = "deep") -> str:
    """ë©”ëª¨ë¦¬ ì¸í…”ë¦¬ì „ìŠ¤ ë³´ê³ ì„œ ìƒì„±"""
    analyzer = MemoryIntelligenceAnalyzer(depth)
    report_data = analyzer.generate_intelligence_content()
    
    # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±
    report_lines = []
    
    # ì œëª©
    report_lines.append(f"# {report_data['title']}")
    report_lines.append(f"**ë¶„ì„ ê¹Šì´**: {report_data['analysis_depth'].upper()}")
    report_lines.append("")
    
    # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
    if report_data["insights"]:
        report_lines.append("## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸")
        for insight in report_data["insights"]:
            report_lines.append(f"- {insight}")
        report_lines.append("")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„
    db_data = report_data["database"]
    report_lines.append("## ğŸ—„ï¸ ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„")
    report_lines.append(f"- **ì´ ë ˆì½”ë“œ**: {db_data['total_records']:,}ê°œ")
    
    if db_data["record_types"]:
        report_lines.append("- **í…Œì´ë¸”ë³„ ë¶„í¬**:")
        for table, count in db_data["record_types"].items():
            report_lines.append(f"  - {table}: {count:,}ê°œ")
    
    if db_data["content_analysis"]:
        report_lines.append("- **ì½˜í…ì¸  ë¶„ì„**:")
        for table, analysis in db_data["content_analysis"].items():
            patterns = analysis.get("content_patterns", {})
            if patterns.get("topic_distribution"):
                top_topics = dict(Counter(patterns["topic_distribution"]).most_common(3))
                report_lines.append(f"  - {table} ì£¼ìš” ì£¼ì œ: {', '.join(f'{k}({v})' for k, v in top_topics.items())}")
    
    for insight in db_data.get("insights", []):
        report_lines.append(f"- {insight}")
    report_lines.append("")
    
    # JSONL ë¶„ì„
    jsonl_data = report_data["jsonl"]
    report_lines.append("## ğŸ“ ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ë¶„ì„")
    report_lines.append(f"- **ë¶„ì„ íŒŒì¼**: {jsonl_data['files_analyzed']}ê°œ")
    report_lines.append(f"- **ì´ ì—”íŠ¸ë¦¬**: {jsonl_data['total_entries']:,}ê°œ")
    report_lines.append(f"- **í•™ìŠµ ì†ë„**: {jsonl_data['learning_velocity']}")
    
    topics = jsonl_data.get("content_insights", {}).get("topic_distribution", {})
    if topics:
        report_lines.append("- **ì£¼ìš” í•™ìŠµ ì£¼ì œ**:")
        for topic, count in list(topics.items())[:5]:
            report_lines.append(f"  - {topic}: {count}ê±´")
    report_lines.append("")
    
    # í•™ìŠµ íŒ¨í„´ ë¶„ì„
    learning_data = report_data["learning"]
    report_lines.append("## ğŸ“ í•™ìŠµ íŒ¨í„´ ë¶„ì„")
    report_lines.append(f"- **í•™ìŠµ íš¨ìœ¨ì„±**: {learning_data['learning_efficiency']}")
    
    retention = learning_data.get("retention_patterns", {})
    if retention:
        report_lines.append("- **ê¸°ì–µ ë³´ì¡´ ìƒíƒœ**:")
        for pattern, status in retention.items():
            status_icon = "âœ…" if status == "ì •ìƒ" else "âš ï¸"
            report_lines.append(f"  - {pattern}: {status_icon} {status}")
    report_lines.append("")
    
    # ê¶Œê³ ì‚¬í•­
    if report_data["recommendations"]:
        report_lines.append("## ğŸ¯ ê°œì„  ê¶Œê³ ì‚¬í•­")
        for rec in report_data["recommendations"]:
            report_lines.append(f"- {rec}")
        report_lines.append("")
    
    # ìƒì„± ì •ë³´
    report_lines.append("---")
    report_lines.append(f"*ìƒì„±ì‹œê°„: {report_data['timestamp']}*")
    report_lines.append("*VELOS ë©”ëª¨ë¦¬ ì¸í…”ë¦¬ì „ìŠ¤ ë³´ê³ ì„œ v1.0*")
    
    return "\n".join(report_lines)

if __name__ == "__main__":
    import sys
    depth = sys.argv[1] if len(sys.argv) > 1 else "deep"
    report_content = generate_memory_intelligence_report(depth)
    print(report_content)